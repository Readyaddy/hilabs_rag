{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import PyPDF2\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import ollama\n",
    "from typing import List, Dict, Tuple, Union\n",
    "import json\n",
    "import tabula\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "class LLMServer:\n",
    "    def __init__(self, model_name='gemma2:2b'):\n",
    "        self.model_name = model_name\n",
    "        self.client = ollama.Client()\n",
    "\n",
    "    def generate_response(self, prompt):\n",
    "        response = self.client.chat(model=self.model_name, messages=[\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': prompt,\n",
    "            },\n",
    "        ])\n",
    "        return response['message']['content']\n",
    "\n",
    "# Initialize the LLM server\n",
    "llm_server = LLMServer()\n",
    "\n",
    "def extract_names_from_bracketed_string(s):\n",
    "    # Regular expression pattern to match terms inside square brackets and separated by commas\n",
    "    pattern = r'\\[([\\w\\s,]+)\\]'\n",
    "    \n",
    "    # Find the content inside the square brackets\n",
    "    match = re.search(pattern, s)\n",
    "    \n",
    "    if match:\n",
    "        # Extract the content within the brackets\n",
    "        content = match.group(1)\n",
    "        # Split the content by commas and strip any extra whitespace\n",
    "        names = [name.strip() for name in content.split(',')]\n",
    "        return names\n",
    "    return None\n",
    "\n",
    "def extract_df(pdf_file):\n",
    "    return tabula.read_pdf(pdf_file, pages='all', multiple_tables=True)\n",
    "def check_code(prompt):\n",
    "    # Regular expression patterns\n",
    "    alphanumeric_pattern = r'\\b[a-zA-Z0-9][0-9]{2,5}\\b'\n",
    "    \n",
    "    # Generate the response from llm_server\n",
    "    names_and_terms_matches = llm_server.generate_response(f\"String:{prompt}If this string contains some specific names like people, disease or for procedure.Output names with separated by commas.Output:[name1,name2].give no explaination\")\n",
    "    print(names_and_terms_matches)\n",
    "    names_and_terms_matches=extract_names_from_bracketed_string(names_and_terms_matches)\n",
    "    # Find alphanumeric strings where the first character is an alphabet\n",
    "    alphanumeric_matches = re.findall(alphanumeric_pattern, prompt)\n",
    "    print(alphanumeric_matches)\n",
    "    # Combine results if either is not empty\n",
    "    result = []\n",
    "    if alphanumeric_matches:\n",
    "        result.extend(alphanumeric_matches)\n",
    "        if names_and_terms_matches:\n",
    "            result.extend(names_and_terms_matches)\n",
    "    \n",
    "    return names_and_terms_matches,', '.join(result) if result else None\n",
    "# def check_code(question):\n",
    "#     prompt = f\"You are an AI classifier. Your job is to determine whether a given question contains a code. A 'code' is defined as any alphanumeric sequence that contains both letters and numbers, such as 'S9582' or 'G15168'. If the question contains a code, respond with the code present. If it does not contain a code, respond with 'no'. Here is the question: {question}\"\n",
    "#     answer = llm_server.generate_response(prompt).strip()\n",
    "#     return answer if answer.lower() != \"no\" else None\n",
    "\n",
    "def search_dataframe_for_code(dfs, codes):\n",
    "    results = []\n",
    "    for code in codes:\n",
    "        for df in dfs:\n",
    "            mask = df.map(lambda x: code.lower() in str(x).lower())\n",
    "            result = df[mask.any(axis=1)]\n",
    "            if not result.empty:\n",
    "                results.append(result)\n",
    "    return results if results else None\n",
    "\n",
    "def extract_text_from_pdf(pdf_file: str) -> str:\n",
    "    with open(pdf_file, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        return \"\\n\".join(page.extract_text() for page in reader.pages)\n",
    "\n",
    "def create_vectorstore(texts: str, folder_name: str) -> Chroma:\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    chunks = text_splitter.split_text(texts)\n",
    "    vectorstore = Chroma.from_texts(\n",
    "        texts=chunks,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=os.path.join(folder_name, \"chroma_db\")\n",
    "    )\n",
    "    vectorstore.persist()\n",
    "    return vectorstore\n",
    "\n",
    "def process_pdf(pdf_file: str) -> Tuple[Chroma, List[pd.DataFrame]]:\n",
    "    folder_name = \"pdf_content\"\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "    print(\"Extracting text from PDF...\")\n",
    "    all_text = extract_text_from_pdf(pdf_file)\n",
    "    \n",
    "    print(\"Extracting tables from PDF...\")\n",
    "    tables = extract_df(pdf_file)\n",
    "    \n",
    "    print(\"Creating vector store...\")\n",
    "    vectorstore = create_vectorstore(all_text, folder_name)\n",
    "    \n",
    "    return vectorstore, tables\n",
    "\n",
    "def word_search(chunks: List[str], keywords: List[str], k: int = 5) -> List[str]:\n",
    "    def count_keywords(chunk):\n",
    "        return sum(keyword.lower() in chunk.lower() for keyword in keywords)\n",
    "    \n",
    "    sorted_chunks = sorted(chunks, key=count_keywords, reverse=True)\n",
    "    return sorted_chunks[:k]\n",
    "\n",
    "def extract_keywords(question: str) -> List[str]:\n",
    "    prompt = f\"Extract keywords from: {question}\\nOutput format: [keyword1,keyword2,keyword3]\"\n",
    "    response = llm_server.generate_response(prompt)\n",
    "    try:\n",
    "        return json.loads(response)\n",
    "    except json.JSONDecodeError:\n",
    "        return question.split()\n",
    "def get_dynamic_k(question: str) -> int:\n",
    "    return min(5, max(1, len(question.split()) // 5))\n",
    "\n",
    "def generate_multiple_queries(original_query: str) -> List[str]:\n",
    "    prompt = f\"\"\"\n",
    "    Given the following user query, generate 3 related queries that could help provide a more comprehensive answer.\n",
    "    Original query: {original_query}\n",
    "    \n",
    "    Output format:\n",
    "    1. [First related query]\n",
    "    2. [Second related query]\n",
    "    3. [Third related query]\n",
    "\n",
    "    Give no explaination.\n",
    "    \"\"\"\n",
    "    response = llm_server.generate_response(prompt)\n",
    "    queries = [original_query]\n",
    "    for line in response.split('\\n'):\n",
    "        if line.strip():\n",
    "            parts = line.split('. ', 1)\n",
    "            if len(parts) > 1:\n",
    "                queries.append(parts[1].strip())\n",
    "    return queries[:4]  # Limit to 4 queries (original + 3 generated)\n",
    "def process_single_query(question: str, vectorstore, tables) -> Dict:\n",
    "\n",
    "    keywords = extract_keywords(question)\n",
    "    print(f\"Keywords extracted: {keywords}\")\n",
    "    k = get_dynamic_k(question)\n",
    "    retrieved_docs = unified_retrieval(vectorstore, question, keywords, k)\n",
    "    \n",
    "    context = \"\\n\".join(retrieved_docs)\n",
    "    prompt = f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer based on the context. If insufficient information, say 'I don't have enough information to answer this question.'\"\n",
    "    answer = llm_server.generate_response(prompt)\n",
    "    \n",
    "    # Determine confidence level\n",
    "    \n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"keywords_used\": keywords,\n",
    "        \"retrieved_documents\": retrieved_docs,\n",
    "        \"source\": \"text\",\n",
    "    }\n",
    "\n",
    "def unified_retrieval(vectorstore: Chroma, question: str, keywords: List[str], k: int = 5) -> List[str]:\n",
    "    similarity_docs = vectorstore.similarity_search(question, k=k)\n",
    "    all_chunks = [doc.page_content for doc in similarity_docs]\n",
    "    \n",
    "    keyword_docs = word_search(all_chunks, keywords, k=k)\n",
    "    \n",
    "    combined_docs = list(set(all_chunks + keyword_docs))\n",
    "    \n",
    "    # Simple ranking based on keyword presence and similarity order\n",
    "    def rank_doc(doc):\n",
    "        keyword_count = sum(keyword.lower() in doc.lower() for keyword in keywords)\n",
    "        similarity_rank = all_chunks.index(doc) if doc in all_chunks else len(all_chunks)\n",
    "        return (keyword_count, -similarity_rank)\n",
    "    \n",
    "    ranked_docs = sorted(combined_docs, key=rank_doc, reverse=True)\n",
    "    return ranked_docs[:k]\n",
    "\n",
    "def get_dynamic_k(question: str) -> int:\n",
    "    return min(5, max(1, len(question.split()) // 5))\n",
    "\n",
    "def process_code_results(question: str, code: str, search_results: List[pd.DataFrame]) -> str:\n",
    "    results_str = \"\\n\".join([df.to_string() for df in search_results])\n",
    "    prompt = f\"\"\"\n",
    "    Question: {question}\n",
    "    Code: {code}\n",
    "    \n",
    "    The following table data was found for the code:\n",
    "    \n",
    "    {results_str}\n",
    "    \n",
    "    Based on this information, please provide an answer to the question. \n",
    "    If you need any clarification or if the information is insufficient, please state so.\n",
    "    \"\"\"\n",
    "    return llm_server.generate_response(prompt)\n",
    "def evaluate_answer_quality(question: str, answer: str) -> bool:\n",
    "    print(answer)\n",
    "    prompt = f\"\"\"\n",
    "    Question: {question}\n",
    "    Answer: {answer}\n",
    "\n",
    "    Evaluate if the given answer addresses the question. \n",
    "    Output: Yes/No\n",
    "    \"\"\"\n",
    "    evaluation = llm_server.generate_response(prompt).strip().lower()\n",
    "    print(evaluation)\n",
    "    return evaluation == 'yes'\n",
    "def ask_question_interface(question: str, global_vectorstore, global_tables) -> str:\n",
    "    if global_vectorstore is None:\n",
    "        return json.dumps({\"error\": \"Please upload and process a PDF first.\"}, indent=2)\n",
    "    \n",
    "    names,code = check_code(question)\n",
    "    if code is not None and global_tables is not None:\n",
    "        search_results = search_dataframe_for_code(global_tables, code)\n",
    "        if search_results:\n",
    "            answer = process_code_results(question, code, search_results)\n",
    "            return json.dumps({\"answer\": answer}, indent=2)\n",
    "    print(names)\n",
    "    if names:\n",
    "        search_results = search_dataframe_for_code(global_tables, names)\n",
    "    keywords = extract_keywords(question)\n",
    "    print(keywords)\n",
    "    k = get_dynamic_k(question)\n",
    "    retrieved_docs = unified_retrieval(global_vectorstore, question, keywords, k)\n",
    "    print(retrieved_docs)\n",
    "    context = \"\\n\".join(retrieved_docs)\n",
    "    results_str='none'\n",
    "    if search_results:\n",
    "     results_str = \"\\n\".join([df.to_string() for df in search_results]) \n",
    "    print(context)\n",
    "    prompt = f\"some extra info:{results_str}Context: {context}\\n\\nQuestion: {question}\\n\\n Answer the question based on the context.\"\n",
    "    answer = llm_server.generate_response(prompt)\n",
    "    q=evaluate_answer_quality(question,answer)\n",
    "    if q:\n",
    "        output = {\n",
    "        \"answer\": answer,\n",
    "        \"keywords_used\": keywords,\n",
    "        \"retrieved_documents\": retrieved_docs\n",
    "       }\n",
    "    else:\n",
    "        # If not good enough, proceed with multiple queries approach\n",
    "        queries = generate_multiple_queries(question)\n",
    "        print(f\"Generated queries: {queries}\")\n",
    "        \n",
    "        results = []\n",
    "        for query in queries:\n",
    "            result = process_single_query(query, global_vectorstore, global_tables)\n",
    "            results.append({\"query\": query, \"result\": result})\n",
    "        \n",
    "        combined_prompt = f\"\"\"\n",
    "        Original question: {question}\n",
    "        \n",
    "        Here are the results from multiple related queries:\n",
    "        \n",
    "        {json.dumps(results, indent=2)}\n",
    "        \n",
    "        Please provide a comprehensive answer to the original question based on all these results.\n",
    "        If there are conflicting information or insufficient data, please mention it.\n",
    "        \"\"\"\n",
    "        \n",
    "        final_answer = llm_server.generate_response(combined_prompt)\n",
    "        \n",
    "        output = {\n",
    "            \"original_question\": question,\n",
    "            \"final_answer\": final_answer,\n",
    "            \"approach\": \"multiple_queries\",\n",
    "            \"individual_results\": results\n",
    "        }\n",
    "    \n",
    "    return json.dumps(output, indent=2)\n",
    "\n",
    "global_vectorstore = None\n",
    "global_tables = None\n",
    "\n",
    "def process_pdf_interface(pdf_file: gr.File) -> str:\n",
    "    global global_vectorstore, global_tables\n",
    "    global_vectorstore, global_tables = process_pdf(pdf_file.name)\n",
    "    return \"PDF processed successfully!\"\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# PDF Chatbot with LLM and Table Extraction\")\n",
    "    \n",
    "    with gr.Tab(\"Process PDF\"):\n",
    "        pdf_file_input = gr.File(label=\"Upload PDF\", file_types=[\".pdf\"])\n",
    "        process_button = gr.Button(\"Process PDF\")\n",
    "        process_output = gr.Textbox(label=\"Processing Result\")\n",
    "        \n",
    "        process_button.click(process_pdf_interface, inputs=pdf_file_input, outputs=process_output)\n",
    "    \n",
    "    with gr.Tab(\"Ask Questions\"):\n",
    "        question_input = gr.Textbox(label=\"Enter your question about the PDF\")\n",
    "        ask_button = gr.Button(\"Ask\")\n",
    "        answer_output = gr.JSON(label=\"Answer\")\n",
    "        \n",
    "        ask_button.click(\n",
    "            lambda q: ask_question_interface(q, global_vectorstore, global_tables),\n",
    "            inputs=question_input,\n",
    "            outputs=answer_output\n",
    "        )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "collabkart",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
